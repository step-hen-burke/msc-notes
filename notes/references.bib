@inproceedings{10.5555/3294996.3295074,
  title = {{{LightGBM}}: {{A}} Highly Efficient Gradient Boosting Decision Tree},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = {2017},
  series = {{{NIPS}}'17},
  pages = {3149--3157},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  isbn = {978-1-5108-6096-4}
}

@article{1b203adf-7660-3d30-b519-ddb7d210b62c,
  title = {Visualising the Demographic Factors Which Shape Population Age Structure},
  author = {Wilson, Tom},
  year = {2016},
  journal = {Demographic Research},
  volume = {35},
  eprint = {26332097},
  eprinttype = {jstor},
  pages = {867--890},
  publisher = {{Max-Planck-Gesellschaft zur Foerderung der Wissenschaften}},
  issn = {14359871, 23637064},
  urldate = {2023-10-10},
  abstract = {BACKGROUND The population pyramid is one of the most popular tools for visualising population age structure. However, it is difficult to discern from the diagram the relative effects of different demographic components on the size of age-specific populations, making it hard to understand exactly how a population's age structure is formed. OBJECTIVE The aim of this paper is to introduce a type of population pyramid which shows how births, deaths, and migration have shaped a population's age structure. DATA AND METHODS Births, deaths, and population data were obtained from the Human Mortality Database and the Australian Bureau of Statistics. A variation on the conventional population pyramid, termed here a components-of-change pyramid, was created. Based on cohort population accounts, it illustrates how births, deaths, and net migration have created the population of each age group. A simple measure which summarises the impact of net migration on age structure is also suggested. RESULTS Example components-of-change pyramids for several countries and subnational regions are presented, which illustrate how births, deaths, and net migration have fashioned current population age structures. The influence of migration is shown to vary greatly between populations. CONCLUSIONS The new type of pyramid aids interpretation of a population's age structure and helps to understand its demographic history over the last century.},
  file = {/home/stephen/Zotero/storage/576NF87A/Wilson - 2016 - Visualising the demographic factors which shape po.pdf}
}

@article{abukmeilSurveyUnsupervisedGenerative2022,
  title = {A {{Survey}} of {{Unsupervised Generative Models}} for {{Exploratory Data Analysis}} and {{Representation Learning}}},
  author = {Abukmeil, Mohanad and Ferrari, Stefano and Genovese, Angelo and Piuri, Vincenzo and Scotti, Fabio},
  year = {2022},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {5},
  pages = {1--40},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3450963},
  urldate = {2023-09-26},
  abstract = {For more than a century, the methods for data representation and the exploration of the intrinsic structures of data have developed remarkably and consist of supervised and unsupervised methods. However, recent years have witnessed the flourishing of big data, where typical dataset dimensions are high and the data can come in messy, incomplete, unlabeled, or corrupted forms. Consequently, discovering the hidden structure buried inside such data becomes highly challenging. From this perspective, exploratory data analysis plays a substantial role in learning the hidden structures that encompass the significant features of the data in an ordered manner by extracting patterns and testing hypotheses to identify anomalies. Unsupervised generative learning models are a class of machine learning models characterized by their potential to reduce the dimensionality, discover the exploratory factors, and learn representations without any predefined labels; moreover, such models can generate the data from the reduced factors' domain. The beginner researchers can find in this survey the recent unsupervised generative learning models for the purpose of data exploration and learning representations; specifically, this article covers three families of methods based on their usage in the era of big data: blind source separation, manifold learning, and neural networks, from shallow to deep architectures.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.889Z},
  file = {/home/stephen/Zotero/storage/NUCDKM3Z/Abukmeil et al. - 2022 - A Survey of Unsupervised Generative Models for Exp.pdf}
}

@article{aliHeartDiseasePrediction2021,
  title = {Heart Disease Prediction Using Supervised Machine Learning Algorithms: {{Performance}} Analysis and Comparison},
  shorttitle = {Heart Disease Prediction Using Supervised Machine Learning Algorithms},
  author = {Ali, Md Mamun and Paul, Bikash Kumar and Ahmed, Kawsar and Bui, Francis M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104672},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2021.104672},
  urldate = {2023-09-20},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.179Z},
  file = {/home/stephen/Zotero/storage/CSG4QWE8/Ali et al. - 2021 - Heart disease prediction using supervised machine .pdf},
  keywords = {MachineLearning,SupervisedLearning}
}

@inproceedings{alnemariEfficientDeepNeural2019,
  title = {Efficient {{Deep Neural Networks}} for {{Edge Computing}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Edge Computing}} ({{EDGE}})},
  author = {Alnemari, Mohammed and Bagherzadeh, Nader},
  year = {2019},
  month = jul,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Milan, Italy}},
  doi = {10.1109/EDGE.2019.00014},
  urldate = {2024-02-27},
  isbn = {978-1-72812-708-8}
}

@misc{apachesoftwarefoundationHttpsParquetApache2023,
  title = {{{https://parquet.apache.org/}}},
  author = {{Apache Software Foundation}},
  year = {2023},
  urldate = {2023-12-30}
}

@article{article,
  title = {Data Structures for Statistical Computing in Python},
  author = {Mckinney, Wes},
  year = {2010},
  month = jan,
  journal = {Proceedings of the 9th Python in Science Conference}
}

@article{article,
  title = {Why Women Live Longer than Men? {{Review}} of Biological and Non-Biological Factors},
  author = {Gryclewska, B.},
  year = {2016},
  month = jan,
  journal = {Przeglad lekarski},
  volume = {73},
  pages = {392--394}
}

@article{article,
  title = {The Visual Display of Quantitative Information / {{E}}.{{R}}. Tufte.},
  author = {Tufte, Edward},
  year = {2001},
  month = jan,
  journal = {American Journal of Physics},
  volume = {31},
  doi = {10.1109/MPER.1988.587534}
}

@misc{ballerDeepEdgeBenchBenchmarkingDeep2021,
  title = {{{DeepEdgeBench}}: {{Benchmarking Deep Neural Networks}} on {{Edge Devices}}},
  shorttitle = {{{DeepEdgeBench}}},
  author = {Baller, Stephan Patrick and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael},
  year = {2021},
  month = aug,
  number = {arXiv:2108.09457},
  eprint = {2108.09457},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-27},
  abstract = {EdgeAI (Edge computing based Artificial Intelligence) has been most actively researched for the last few years to handle variety of massively distributed AI applications to meet up the strict latency requirements. Meanwhile, many companies have released edge devices with smaller form factors (low power consumption and limited resources) like the popular Raspberry Pi and Nvidia's Jetson Nano for acting as compute nodes at the edge computing environments. Although the edge devices are limited in terms of computing power and hardware resources, they are powered by accelerators to enhance their performance behavior. Therefore, it is interesting to see how AI-based Deep Neural Networks perform on such devices with limited resources. In this work, we present and compare the performance in terms of inference time and power consumption of the four Systems on a Chip (SoCs): Asus Tinker Edge R, Raspberry Pi 4, Google Coral Dev Board, Nvidia Jetson Nano, and one microcontroller: Arduino Nano 33 BLE, on different deep learning models and frameworks. We also provide a method for measuring power consumption, inference time and accuracy for the devices, which can be easily extended to other devices. Our results showcase that, for Tensorflow based quantized model, the Google Coral Dev Board delivers the best performance, both for inference time and power consumption. For a low fraction of inference computation time, i.e. less than 29.3\% of the time for MobileNetV2, the Jetson Nano performs faster than the other devices.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/NE43DVRD/Baller et al. - 2021 - DeepEdgeBench Benchmarking Deep Neural Networks o.pdf;/home/stephen/Zotero/storage/IC2B6F5Q/2108.html},
  keywords = {ComputerScience-ArtificialIntelligence,ComputerScience-DistributedParallelAndClusterComputing}
}

@misc{ballerDeepEdgeBenchBenchmarkingDeep2021a,
  title = {{{DeepEdgeBench}}: {{Benchmarking Deep Neural Networks}} on {{Edge Devices}}},
  shorttitle = {{{DeepEdgeBench}}},
  author = {Baller, Stephan Patrick and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael},
  year = {2021},
  month = aug,
  number = {arXiv:2108.09457},
  eprint = {2108.09457},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-27},
  abstract = {EdgeAI (Edge computing based Artificial Intelligence) has been most actively researched for the last few years to handle variety of massively distributed AI applications to meet up the strict latency requirements. Meanwhile, many companies have released edge devices with smaller form factors (low power consumption and limited resources) like the popular Raspberry Pi and Nvidia's Jetson Nano for acting as compute nodes at the edge computing environments. Although the edge devices are limited in terms of computing power and hardware resources, they are powered by accelerators to enhance their performance behavior. Therefore, it is interesting to see how AI-based Deep Neural Networks perform on such devices with limited resources. In this work, we present and compare the performance in terms of inference time and power consumption of the four Systems on a Chip (SoCs): Asus Tinker Edge R, Raspberry Pi 4, Google Coral Dev Board, Nvidia Jetson Nano, and one microcontroller: Arduino Nano 33 BLE, on different deep learning models and frameworks. We also provide a method for measuring power consumption, inference time and accuracy for the devices, which can be easily extended to other devices. Our results showcase that, for Tensorflow based quantized model, the Google Coral Dev Board delivers the best performance, both for inference time and power consumption. For a low fraction of inference computation time, i.e. less than 29.3\% of the time for MobileNetV2, the Jetson Nano performs faster than the other devices.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/2JNB6TFR/Baller et al. - 2021 - DeepEdgeBench Benchmarking Deep Neural Networks o.pdf;/home/stephen/Zotero/storage/3958ZZU2/2108.html},
  keywords = {ComputerScience-ArtificialIntelligence,ComputerScience-DistributedParallelAndClusterComputing}
}

@article{bergmeirUseCrossvalidationTime2012,
  title = {On the Use of Cross-Validation for Time Series Predictor Evaluation},
  author = {Bergmeir, Christoph and Ben{\'i}tez, Jos{\'e} M.},
  year = {2012},
  month = may,
  journal = {Information Sciences},
  volume = {191},
  pages = {192--213},
  issn = {00200255},
  doi = {10.1016/j.ins.2011.12.028},
  urldate = {2023-11-03},
  langid = {english}
}

@article{Bertsimas2017FromPM,
  title = {From Predictive Methods to Missing Data Imputation: {{An}} Optimization Approach},
  author = {Bertsimas, Dimitris and Pawlowski, Colin and Zhuo, Ying Daisy},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {196:1-196:39},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.893Z},
  file = {/home/stephen/Zotero/storage/X8HSFH2E/Bertsimas et al. - 2017 - From predictive methods to missing data imputation.pdf}
}

@misc{buitinckAPIDesignMachine2013,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  shorttitle = {{{API}} Design for Machine Learning Software},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  year = {2013},
  month = sep,
  number = {arXiv:1309.0238},
  eprint = {1309.0238},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/6H4539S5/Buitinck et al. - 2013 - API design for machine learning software experien.pdf;/home/stephen/Zotero/storage/QCNF79QH/API design for machine learning software.pdf;/home/stephen/Zotero/storage/GJDK7Q48/1309.html},
  keywords = {APIDesign,MachineLearning}
}

@book{burgerIntroductionMachineLearning2018,
  title = {Introduction to Machine Learning with {{R}}: Rigorous Mathematical Analysis},
  shorttitle = {Introduction to Machine Learning with {{R}}},
  author = {Burger, Scott V.},
  year = {2018},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Beijing Boston Farnham Sebastopol Tokyo}},
  abstract = {Machine learning can be a difficult subject if you're not familiar with the basics. With this book, you'll get a solid foundation of introductory principles used in machine learning with the statistical programming language R. You'll start with the basics like regression, then move into more advanced topics like neural networks, and finally delve into the frontier of machine learning in the R world with packages like Caret. By developing a familiarity with topics like understanding the difference between regression and classification models, you'll be able to solve an array of machine learning problems. Knowing when to use a specific model or not can mean the difference between a highly accurate model and a completely useless one. This book provides copious examples to build a working knowledge of machine learning. Understand the major parts of machine learning algorithms Recognize how machine learning can be used to solve a problem in a simple manner Figure out when to use certain machine learning algorithms versus others Learn how to operationalize algorithms with cutting edge packages},
  isbn = {978-1-4919-7644-9},
  langid = {english},
  file = {/home/stephen/Zotero/storage/9B2CTQJF/Scott V. Burger - Introduction to Machine Learning with R. Rigorous Mathematical Analysis-O’Reilly (2018).pdf;/home/stephen/Zotero/storage/X2QPID99/Burger - 2018 - Introduction to machine learning with R rigorous .pdf}
}

@book{burkovHundredpageMachineLearning2019,
  title = {The Hundred-Page Machine Learning Book},
  author = {Burkov, Andriy},
  year = {2019},
  publisher = {{Andriy Burkov}},
  address = {{Polen}},
  isbn = {978-1-9995795-0-0 978-1-9995795-1-7},
  langid = {english}
}

@book{burnhamModelSelectionMultimodel2002,
  title = {Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach},
  shorttitle = {Model Selection and Multimodel Inference},
  author = {Burnham, Kenneth P. and Anderson, David Raymond and Burnham, Kenneth P.},
  year = {2002},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-95364-9},
  lccn = {QH323.5 .B87 2002},
  annotation = {OCLC: ocm48557578},
  keywords = {Biology,MathematicalModels,MathematicalStatistics}
}

@misc{centralstatisticsofficeCensus2022Population2023,
  title = {Census 2022 {{Population Grid}} ({{https://ie-cso.maps.arcgis.com/apps/webappviewer/index.html?id=0fe164e96d254776866425e2fd3e73af)}}},
  author = {{Central Statistics Office}},
  year = {2023},
  urldate = {2023-12-30}
}

@book{chatfieldAnalysisTimeSeries2003,
  title = {The {{Analysis}} of {{Time Series}}},
  author = {Chatfield, Chris},
  year = {2003},
  month = jul,
  edition = {0},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.4324/9780203491683},
  urldate = {2023-11-03},
  isbn = {978-0-203-49168-3},
  langid = {english}
}

@article{chenComparisonLinearRegression2019,
  title = {A Comparison of Linear Regression, Regularization, and Machine Learning Algorithms to Develop {{Europe-wide}} Spatial Models of Fine Particles and Nitrogen Dioxide},
  author = {Chen, Jie and De Hoogh, Kees and Gulliver, John and Hoffmann, Barbara and Hertel, Ole and Ketzel, Matthias and Bauwelinck, Mariska and Van Donkelaar, Aaron and Hvidtfeldt, Ulla A. and Katsouyanni, Klea and Janssen, Nicole A.H. and Martin, Randall V. and Samoli, Evangelia and Schwartz, Per E. and Stafoggia, Massimo and Bellander, Tom and Strak, Maciek and Wolf, Kathrin and Vienneau, Danielle and Vermeulen, Roel and Brunekreef, Bert and Hoek, Gerard},
  year = {2019},
  month = sep,
  journal = {Environment International},
  volume = {130},
  pages = {104934},
  issn = {01604120},
  doi = {10.1016/j.envint.2019.104934},
  urldate = {2023-10-14},
  langid = {english},
  file = {/home/stephen/Zotero/storage/7BYV3XHJ/1-s2.0-S0160412019304404-main.pdf;/home/stephen/Zotero/storage/7SGFJ7I2/Chen et al. - 2019 - A comparison of linear regression, regularization,.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2024-01-03},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/ZQEAAMWK/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;/home/stephen/Zotero/storage/NT4H2GMI/1603.html},
  keywords = {ComputerScience-MachineLearning}
}

@article{chiangInfoCleanProtectingSensitive2017,
  title = {{{InfoClean}}: {{Protecting Sensitive Information}} in {{Data Cleaning}}},
  shorttitle = {{{InfoClean}}},
  author = {Chiang, Fei and Gairola, Dhruv},
  year = {2017},
  month = dec,
  journal = {Journal of Data and Information Quality},
  volume = {9},
  number = {4},
  pages = {1--26},
  issn = {1936-1955, 1936-1963},
  doi = {10.1145/3190577},
  urldate = {2023-09-26},
  abstract = {Data quality has become a pervasive challenge for organizations as they wrangle with large, heterogeneous datasets to extract value. Given the proliferation of sensitive and confidential information, it is crucial to consider data privacy concerns during the data cleaning process. For example, in medical database applications, varying levels of privacy are enforced across the attribute values. Attributes such as a patient's country or city of residence may be less sensitive than the patient's prescribed medication. Traditional data cleaning techniques assume the data is openly accessible, without considering the differing levels of information sensitivity. In this work, we take the first steps toward a data cleaning model that integrates privacy as part of the data cleaning process. We present a privacy-aware data cleaning framework that differentiates the information content among the attribute values during the data cleaning process to resolve data inconsistencies while minimizing the amount of information disclosed. Our data repair algorithm includes a set of data disclosure operations that considers the information content of the underlying attribute values, while maximizing data utility. Our evaluation using real datasets shows that our algorithm scales well, and achieves improved performance and comparable repair accuracy against existing data cleaning solutions.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.890Z},
  file = {/home/stephen/Zotero/storage/H3I4K7ZC/Chiang and Gairola - 2017 - InfoClean Protecting Sensitive Information in Dat.pdf}
}

@article{chuDistributedDataDeduplication2016,
  title = {Distributed Data Deduplication},
  author = {Chu, Xu and Ilyas, Ihab F. and Koutris, Paraschos},
  year = {2016},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {11},
  pages = {864--875},
  issn = {2150-8097},
  doi = {10.14778/2983200.2983203},
  urldate = {2023-09-27},
  abstract = {Data deduplication refers to the process of identifying tuples in a relation that refer to the same real world entity. The complexity of the problem is inherently quadratic with respect to the number of tuples, since a similarity value must be computed for every pair of tuples. To avoid comparing tuple pairs that are obviously non-duplicates, blocking techniques are used to divide the tuples into blocks and only tuples within the same block are compared. However, even with the use of blocking, data deduplication remains a costly problem for large datasets. In this paper, we show how to further speed up data deduplication by leveraging parallelism in a shared-nothing computing environment. Our main contribution is a distribution strategy, called Dis-Dedup, that minimizes the maximum workload across all worker nodes and provides strong theoretical guarantees. We demonstrate the effectiveness of our proposed strategy by performing extensive experiments on both synthetic datasets with varying block size distributions, as well as real world datasets.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/22QX8R32/Chu et al. - 2016 - Distributed data deduplication.pdf}
}

@article{chuQualitativeDataCleaning2016,
  title = {Qualitative Data Cleaning},
  author = {Chu, Xu and Ilyas, Ihab F.},
  year = {2016},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {13},
  pages = {1605--1608},
  issn = {2150-8097},
  doi = {10.14778/3007263.3007320},
  urldate = {2023-09-26},
  abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.             Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.             In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning "big data" in terms of scale and distribution.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.891Z},
  file = {/home/stephen/Zotero/storage/XP984QYI/Chu and Ilyas - 2016 - Qualitative data cleaning.pdf}
}

@misc{cso2016Census,
  title = {2016 {{Census}}},
  author = {CSO}
}

@misc{cuiDataSiteProactiveVisual2018,
  title = {{{DataSite}}: {{Proactive Visual Data Exploration}} with {{Computation}} of {{Insight-based Recommendations}}},
  shorttitle = {{{DataSite}}},
  author = {Cui, Zhe and Badam, Sriram Karthik and Yal{\c c}in, Adil and Elmqvist, Niklas},
  year = {2018},
  month = sep,
  number = {arXiv:1802.08621},
  eprint = {1802.08621},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/IP6X7PCH/DataSite  Proactive Visual Data.pdf;/home/stephen/Zotero/storage/L4SIDND9/Cui et al. - 2018 - DataSite Proactive Visual Data Exploration with C.pdf;/home/stephen/Zotero/storage/GFY3TLZD/1802.html},
  keywords = {AutomatedInsights,DataExploration,Recommendation}
}

@book{dangetiStatisticsMachineLearning2017,
  title = {Statistics for Machine Learning: Techniques for Exploring Supervised, Unsupervised, and Reinforcement Learning Models with {{Python}} and {{R}}},
  shorttitle = {Statistics for Machine Learning},
  author = {Dangeti, Pratap},
  year = {2017},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, UK}},
  isbn = {978-1-78829-575-8},
  langid = {english},
  file = {/home/stephen/Zotero/storage/ABA8TNW4/Dangeti - 2017 - Statistics for machine learning techniques for ex.pdf}
}

@article{dengEfficientKNNClassification2016,
  title = {Efficient {{kNN}} Classification Algorithm for Big Data},
  author = {Deng, Zhenyun and Zhu, Xiaoshu and Cheng, Debo and Zong, Ming and Zhang, Shichao},
  year = {2016},
  month = jun,
  journal = {Neurocomputing},
  volume = {195},
  pages = {143--148},
  issn = {09252312},
  doi = {10.1016/j.neucom.2015.08.112},
  urldate = {2023-09-26},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.887Z},
  file = {/home/stephen/Documents/college/research/papers/Efficient kNN classification algorithm for big data.pdf}
}

@article{deutchExplainEDExplanationsEDA2020,
  title = {{{ExplainED}}: Explanations for {{EDA}} Notebooks},
  shorttitle = {{{ExplainED}}},
  author = {Deutch, Daniel and Gilad, Amir and Milo, Tova and Somech, Amit},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {2917--2920},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415508},
  urldate = {2023-09-22},
  abstract = {Exploratory Data Analysis (EDA) is an essential yet highly demanding task. To get a head start before exploring a new dataset, data scientists often prefer to view existing               EDA notebooks               - illustrative exploratory sessions that were created by fellow data scientists who examined the same dataset and shared their notebooks via online platforms. Unfortunately, creating an illustrative, well-documented notebook is cumbersome and time-consuming, therefore users sometimes share their notebook without explaining their exploratory steps and their results. Such notebooks are difficult to follow and to understand.                          To address this, we present ExplainED, a system that automatically attaches explanations to views in EDA notebooks. ExplainED analyzes each view in order to detect what elements thereof are particularly interesting, and produces a corresponding textual explanation. The explanations are generated by first evaluating the interestingness of the given view using several measures capturing different interestingness facets, then computing the Shapely values of the elements in the view, w.r.t. the interestingness measure yielding the highest score. These Shapely values are then used to guide the generation of the textual explanation.             We demonstrate the usefulness of the explanations generated by ExplainED on real-life, undocumented EDA notebooks.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/T4K936ED/Deutch et al. - 2020 - ExplainED explanations for EDA notebooks.pdf},
  keywords = {EDA}
}

@inproceedings{dingQuickInsightsQuickAutomatic2019,
  title = {{{QuickInsights}}: {{Quick}} and {{Automatic Discovery}} of {{Insights}} from {{Multi-Dimensional Data}}},
  shorttitle = {{{QuickInsights}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Ding, Rui and Han, Shi and Xu, Yong and Zhang, Haidong and Zhang, Dongmei},
  year = {2019},
  month = jun,
  pages = {317--332},
  publisher = {{ACM}},
  address = {{Amsterdam Netherlands}},
  doi = {10.1145/3299869.3314037},
  urldate = {2023-09-22},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {/home/stephen/Zotero/storage/XKUH44Z2/Ding et al. - 2019 - QuickInsights Quick and Automatic Discovery of In.pdf},
  keywords = {AutomatedInsights}
}

@book{downeyThinkStats2012,
  title = {Think Stats},
  author = {Downey, Allen B.},
  year = {2012},
  series = {Probability and Statistics for Programmers},
  edition = {1. ed., 4. release},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4493-0711-0},
  langid = {english},
  file = {/home/stephen/Zotero/storage/J8WTXV5R/Downey - 2012 - Think stats.pdf}
}

@book{efronIntroductionBootstrap1994,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R.J.},
  year = {1994},
  month = may,
  edition = {0},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9780429246593},
  urldate = {2023-10-16},
  isbn = {978-0-429-24659-3},
  langid = {english}
}

@misc{eurostatGISCOGridsHttps2023,
  title = {{{GISCO}} Grids ({{https://gisco-services.ec.europa.eu/grid/GISCO\_grid\_metadata.pdf)}}},
  author = {{eurostat}},
  year = {2023},
  urldate = {2023-12-30}
}

@book{frostjimRegressionAnalysisIntuitive2020,
  title = {Regression {{Analysis}}: {{An Intuitive Guide}} for {{Using}} and {{Interpreting Linear Models}}},
  author = {Frost, Jim},
  year = {2020},
  isbn = {978-1-73543-119-2}
}

@book{geronHandsonMachineLearning2019,
  title = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: Concepts, Tools, and Techniques to Build Intelligent Systems},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2019},
  edition = {Second edition},
  publisher = {{O'Reilly Media, Inc}},
  address = {{Beijing [China] ; Sebastopol, CA}},
  isbn = {978-1-4920-3264-9},
  lccn = {QA76.73.P98 G45 2019},
  file = {/home/stephen/Zotero/storage/E8USP4M9/Géron - 2019 - Hands-on machine learning with Scikit-Learn, Keras.pdf},
  keywords = {ArtificialIntelligence,MachineLearning,Python(ComputerProgramLanguage),TensorFlow}
}

@book{hainesModernDataEngineering2022,
  title = {Modern {{Data Engineering}} with {{Apache Spark}}: {{A Hands-On Guide}} for {{Building Mission-Critical Streaming Applications}}},
  shorttitle = {Modern {{Data Engineering}} with {{Apache Spark}}},
  author = {Haines, Scott},
  year = {2022},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-7452-1},
  urldate = {2024-03-05},
  isbn = {978-1-4842-7451-4 978-1-4842-7452-1},
  langid = {english},
  file = {/home/stephen/Zotero/storage/VNEJEXKE/Haines - 2022 - Modern Data Engineering with Apache Spark A Hands.pdf}
}

@article{hassanatSolvingProblemParameter2014,
  title = {Solving the {{Problem}} of the {{K Parameter}} in the {{KNN Classifier Using}} an {{Ensemble Learning Approach}}},
  author = {Hassanat, Ahmad Basheer and Abbadi, Mohammad Ali and Altarawneh, Ghada Awad and Alhasanat, Ahmad Ali},
  year = {2014},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1409.0919},
  urldate = {2023-09-28},
  abstract = {This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/home/stephen/Zotero/storage/XEDXSKBP/Hassanat et al. - 2014 - Solving the Problem of the K Parameter in the KNN .pdf},
  keywords = {FOS:ComputerAndInformationSciences,MachineLearning(Cs.LG)}
}

@book{hastieElementsStatisticalLearning2017,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  shorttitle = {The Elements of Statistical Learning},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
  year = {2017},
  series = {Springer Series in Statistics},
  edition = {Second edition, corrected at 12th printing 2017},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/b94608},
  isbn = {978-0-387-84857-0},
  langid = {english},
  file = {/home/stephen/Zotero/storage/HH27FM6V/Hastie et al. - 2017 - The elements of statistical learning data mining,.pdf}
}

@inproceedings{hePyramidEnablingHierarchical2022,
  title = {Pyramid: {{Enabling Hierarchical Neural Networks}} with {{Edge Computing}}},
  shorttitle = {Pyramid},
  booktitle = {Proceedings of the {{ACM Web Conference}} 2022},
  author = {He, Qiang and Dong, Zeqian and Chen, Feifei and Deng, Shuiguang and Liang, Weifa and Yang, Yun},
  year = {2022},
  month = apr,
  pages = {1860--1870},
  publisher = {{ACM}},
  address = {{Virtual Event, Lyon France}},
  doi = {10.1145/3485447.3511990},
  urldate = {2024-02-27},
  isbn = {978-1-4503-9096-5},
  langid = {english}
}

@techreport{huddeItMobilityCulture2022,
  type = {Preprint},
  title = {It's the Mobility Culture, Stupid! {{Winter}} Conditions Strongly Reduce Bicycle Usage in {{German}} Cities, but Not in {{Dutch}} Ones},
  author = {Hudde, Ansgar},
  year = {2022},
  month = may,
  institution = {{SocArXiv}},
  doi = {10.31235/osf.io/yejxf},
  urldate = {2024-01-03},
  abstract = {Cycling is healthy, cheap, and environmentally sustainable, but these benefits remain unexploited if many people only cycle in summer but not in winter. Seasonal differences in cycling are small in Dutch cities but relatively large in German cities. This paper tests: is it because seasonal conditions are more moderate in the Netherlands or because German cyclists are more sensitive to temperature, rain, and daylight hours? After discussing how German culture mirrors and potentially perpetuates seasonal cycling patterns, I analyse around 335,000 trips from German and Dutch travel surveys enriched with city-level climate data. Results show that Germans react about twice as strongly to changes in temperature and daylight hours than the Dutch. This reveals untapped potential for year-round cycling in Germany, with its benefits for health, finances and sustainability: The barrier is not the natural environment, but people's response to it, which could potentially change and be influenced by policy.},
  file = {/home/stephen/Zotero/storage/HJ2WJWKZ/Hudde - 2022 - It's the mobility culture, stupid! Winter conditio.pdf}
}

@misc{huttoVaderSentimentHttpsVadersentiment2023,
  title = {{{VaderSentiment}} ({{https://vadersentiment.readthedocs.io/en/latest/)}}},
  author = {Hutto, {\relax CJ}},
  year = {2023},
  urldate = {2023-12-30}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  urldate = {2023-09-20},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  file = {/home/stephen/Zotero/storage/GZYJ4IKC/(Springer Texts in Statistics) Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning with Applications in R-Springer (2014).pdf;/home/stephen/Zotero/storage/IBSYJFME/James et al. - 2013 - An Introduction to Statistical Learning.pdf}
}

@book{jaynesProbabilityTheoryLogic2021,
  title = {Probability Theory: The Logic of Science},
  shorttitle = {Probability Theory},
  author = {Jaynes, Edwin T.},
  editor = {Bretthorst, G. L.},
  year = {2021},
  edition = {24th printing},
  publisher = {{Cambridge Univ. Press}},
  address = {{Cambridge}},
  isbn = {978-0-521-59271-0},
  langid = {english}
}

@misc{jcdecauxSelfserviceBicyclesOpen2023,
  title = {Self-Service Bicycles {{Open Data}} ({{https://developer.jcdecaux.com/\#/opendata/vls?page=static\&contract=dublin)}}},
  author = {{JCDecaux}},
  year = {2023},
  urldate = {2023-12-30}
}

@article{jinHighperformanceIoTStreaming2020,
  title = {High-Performance {{IoT}} Streaming Data Prediction System Using {{Spark}}: A Case Study of Air Pollution},
  shorttitle = {High-Performance {{IoT}} Streaming Data Prediction System Using {{Spark}}},
  author = {Jin, Ho-Yong and Jung, Eun-Sung and Lee, Duckki},
  year = {2020},
  month = sep,
  journal = {Neural Computing and Applications},
  volume = {32},
  number = {17},
  pages = {13147--13154},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-019-04678-9},
  urldate = {2024-03-05},
  langid = {english}
}

@article{koumarelasDataPreparationDuplicate2020,
  title = {Data {{Preparation}} for {{Duplicate Detection}}},
  author = {Koumarelas, Ioannis and Jiang, Lan and Naumann, Felix},
  year = {2020},
  month = sep,
  journal = {Journal of Data and Information Quality},
  volume = {12},
  number = {3},
  pages = {1--24},
  issn = {1936-1955, 1936-1963},
  doi = {10.1145/3377878},
  urldate = {2023-09-20},
  abstract = {Data errors represent a major issue in most application workflows. Before any important task can take place, a certain data quality has to be guaranteed by eliminating a number of different errors that may appear in data. Typically, most of these errors are fixed with data preparation methods, such as whitespace removal. However, the particular error of duplicate records, where multiple records refer to the same entity, is usually eliminated independently with specialized techniques. Our work is the first to bring these two areas together by applying data preparation operations under a systematic approach prior to performing duplicate detection.             Our process workflow can be summarized as follows: It begins with the user providing as input a sample of the gold standard, the actual dataset, and optionally some constraints to domain-specific data preparations, such as address normalization. The preparation selection operates in two consecutive phases. First, to vastly reduce the search space of ineffective data preparations, decisions are made based on the improvement or worsening of pair similarities. Second, using the remaining data preparations an iterative leave-one-out classification process removes preparations one by one and determines the redundant preparations based on the achieved area under the precision-recall curve (AUC-PR). Using this workflow, we manage to improve the results of duplicate detection up to 19\% in AUC-PR.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.176Z},
  file = {/home/stephen/Zotero/storage/2Q24ST9L/Koumarelas et al. - 2020 - Data Preparation for Duplicate Detection.pdf},
  keywords = {DataCleaning,EDA}
}

@book{laroseDiscoveringKnowledgeData2005,
  title = {Discovering Knowledge in Data: An Introduction to Data Mining},
  shorttitle = {Discovering Knowledge in Data},
  author = {Larose, Daniel T.},
  year = {2005},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, NJ}},
  isbn = {978-0-471-66657-8},
  langid = {english},
  file = {/home/stephen/Zotero/storage/699LC4WH/(Wiley Series on Methods and Applications in Data Mining) Daniel T. Larose, Chantel D. Larose - Discovering Knowledge in data An Introduction to Data Mining-Wiley ( 2014).pdf;/home/stephen/Zotero/storage/6MN277H8/Larose - 2005 - Discovering knowledge in data an introduction to .pdf}
}

@article{liBayesianApproachEstimating2009,
  title = {A {{Bayesian Approach}} for {{Estimating}} and {{Replacing Missing Categorical Data}}},
  author = {Li, Xiaobai},
  year = {2009},
  journal = {ACM J. Data Inf. Qual.},
  volume = {1},
  pages = {3:1-3:11},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.175Z},
  file = {/home/stephen/Zotero/storage/ZAHZDSMS/Li - 2009 - A Bayesian Approach for Estimating and Replacing M.pdf},
  keywords = {Bayesian,CategoricalData,DataCleaning,Imputation,MissingData}
}

@inproceedings{linBigIN4InstantInteractive2018,
  title = {{{BigIN4}}: {{Instant}}, {{Interactive Insight Identification}} for {{Multi-Dimensional Big Data}}},
  shorttitle = {{{BigIN4}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Lin, Qingwei and Ke, Weichen and Lou, Jian-Guang and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Zhou, Ziyi and Qiao, Bo and Zhang, Dongmei},
  year = {2018},
  month = jul,
  pages = {547--555},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3219867},
  urldate = {2023-09-22},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  file = {/home/stephen/Zotero/storage/VLQLTFXM/Lin et al. - 2018 - BigIN4 Instant, Interactive Insight Identificatio.pdf},
  keywords = {AutomatedInsights}
}

@article{linweiOptimizedAprioriAlgorithm2023,
  title = {Optimized {{Apriori}} Algorithm for Deformation Response Analysis of Landslide Hazards},
  author = {Linwei, Li and Yiping, Wu and Yepiao, Huang and Bo, Li and Fasheng, Miao and Ziqiang, Deng},
  year = {2023},
  month = jan,
  journal = {Computers \& Geosciences},
  volume = {170},
  pages = {105261},
  issn = {00983004},
  doi = {10.1016/j.cageo.2022.105261},
  urldate = {2023-11-28},
  langid = {english}
}

@article{liRandomSketchLearning2021,
  title = {Random Sketch Learning for Deep Neural Networks in Edge Computing},
  author = {Li, Bin and Chen, Peijun and Liu, Hongfu and Guo, Weisi and Cao, Xianbin and Du, Junzhao and Zhao, Chenglin and Zhang, Jun},
  year = {2021},
  month = mar,
  journal = {Nature Computational Science},
  volume = {1},
  number = {3},
  pages = {221--228},
  issn = {2662-8457},
  doi = {10.1038/s43588-021-00039-6},
  urldate = {2024-02-27},
  langid = {english},
  file = {/home/stephen/Zotero/storage/EIFNMI7R/Li et al. - 2021 - Random sketch learning for deep neural networks in.pdf}
}

@misc{londondatastoreCycleHireAvailability2023,
  title = {Cycle {{Hire Availability}} ({{https://data.london.gov.uk/dataset/cycle-hire-availability)}}},
  author = {{London Datastore}},
  year = {2023},
  urldate = {2023-12-30}
}

@misc{londondatastoreNumberBicycleHires2023,
  title = {Number of {{Bicycle Hires}} ({{https://data.london.gov.uk/dataset/number-bicycle-hires)}}},
  author = {{London Datastore}},
  year = {2023},
  urldate = {2023-12-30}
}

@article{lumleyImportanceNormalityAssumption2002,
  title = {The {{Importance}} of the {{Normality Assumption}} in {{Large Public Health Data Sets}}},
  author = {Lumley, Thomas and Diehr, Paula and Emerson, Scott and Chen, Lu},
  year = {2002},
  month = may,
  journal = {Annual Review of Public Health},
  volume = {23},
  number = {1},
  pages = {151--169},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev.publhealth.23.100901.140546},
  urldate = {2024-01-04},
  abstract = {▪ Abstract{\enspace} It is widely but incorrectly believed that the t-test and linear regression are valid only for Normally distributed outcomes. The t-test and linear regression compare the mean of an outcome variable for different subjects. While these are valid even in very small samples if the outcome variable is Normally distributed, their major usefulness comes from the fact that in large samples they are valid for any distribution. We demonstrate this validity by simulation in extremely non-Normal data. We discuss situations in which in other methods such as the Wilcoxon rank sum test and ordinal logistic regression (proportional odds model) have been recommended, and conclude that the t-test and linear regression often provide a convenient and practical alternative. The major limitation on the t-test and linear regression for inference about associations is not a distributional one, but whether detecting and estimating a difference in the mean of the outcome answers the scientific question at hand.},
  langid = {english}
}

@book{martinCleanCodeHandbook2012,
  title = {Clean Code: A Handbook of Agile Software Craftsmanship},
  shorttitle = {Clean Code},
  author = {Martin, Robert C.},
  year = {2012},
  series = {Robert {{C}}. {{Martin}} Series},
  edition = {Repr.},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River, NJ Munich}},
  isbn = {978-0-13-235088-4},
  langid = {english}
}

@misc{meteireannHistoricalWeatherData2023,
  title = {Historical {{Weather Data}} ({{https://www.met.ie/climate/available-data/historical-data)}}},
  author = {{Met Eireann}},
  year = {2023},
  urldate = {2023-12-30}
}

@misc{mohammadiDeepLearningIoT2018,
  title = {Deep {{Learning}} for {{IoT Big Data}} and {{Streaming Analytics}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{IoT Big Data}} and {{Streaming Analytics}}},
  author = {Mohammadi, Mehdi and {Al-Fuqaha}, Ala and Sorour, Sameh and Guizani, Mohsen},
  year = {2018},
  month = jun,
  number = {arXiv:1712.04301},
  eprint = {1712.04301},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-03-05},
  abstract = {In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely Deep Learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/YX2QQ3DN/Mohammadi et al. - 2018 - Deep Learning for IoT Big Data and Streaming Analy.pdf;/home/stephen/Zotero/storage/6T53WNMS/1712.html},
  keywords = {ComputerScience-Databases,ComputerScience-MachineLearning,ComputerScience-NetworkingAndInternetArchitecture}
}

@article{moritzComparisonDifferentMethods2015,
  title = {Comparison of Different {{Methods}} for {{Univariate Time Series Imputation}} in {{R}}},
  author = {Moritz, Steffen and Sard{\'a}, Alexis and {Bartz-Beielstein}, Thomas and Zaefferer, Martin and Stork, J{\"o}rg},
  year = {2015},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1510.03924},
  urldate = {2023-10-09},
  abstract = {Missing values in datasets are a well-known problem and there are quite a lot of R packages offering imputation functions. But while imputation in general is well covered within R, it is hard to find functions for imputation of univariate time series. The problem is, most standard imputation techniques can not be applied directly. Most algorithms rely on inter-attribute correlations, while univariate time series imputation needs to employ time dependencies. This paper provides an overview of univariate time series imputation in general and an in-detail insight into the respective implementations within R packages. Furthermore, we experimentally compare the R functions on different time series using four different ratios of missing data. Our results show that either an interpolation with seasonal kalman filter from the zoo package or a linear interpolation on seasonal loess decomposed data from the forecast package were the most effective methods for dealing with missing data in most of the scenarios assessed in this paper.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Applications(Stat.AP),FOS:ComputerAndInformationSciences,OtherComputerScience(Cs.OH)}
}

@misc{moritzComparisonDifferentMethods2015a,
  title = {Comparison of Different {{Methods}} for {{Univariate Time Series Imputation}} in {{R}}},
  author = {Moritz, Steffen and Sard{\'a}, Alexis and {Bartz-Beielstein}, Thomas and Zaefferer, Martin and Stork, J{\"o}rg},
  year = {2015},
  month = oct,
  number = {arXiv:1510.03924},
  eprint = {1510.03924},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-09},
  abstract = {Missing values in datasets are a well-known problem and there are quite a lot of R packages offering imputation functions. But while imputation in general is well covered within R, it is hard to find functions for imputation of univariate time series. The problem is, most standard imputation techniques can not be applied directly. Most algorithms rely on inter-attribute correlations, while univariate time series imputation needs to employ time dependencies. This paper provides an overview of univariate time series imputation in general and an in-detail insight into the respective implementations within R packages. Furthermore, we experimentally compare the R functions on different time series using four different ratios of missing data. Our results show that either an interpolation with seasonal kalman filter from the zoo package or a linear interpolation on seasonal loess decomposed data from the forecast package were the most effective methods for dealing with missing data in most of the scenarios assessed in this paper.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/MFDBITS8/Moritz et al. - 2015 - Comparison of different Methods for Univariate Tim.pdf;/home/stephen/Zotero/storage/9HBFHBJR/1510.html},
  keywords = {ComputerScience-OtherComputerScience,Statistics-Applications}
}

@misc{moritzComparisonDifferentMethods2015b,
  title = {Comparison of Different {{Methods}} for {{Univariate Time Series Imputation}} in {{R}}},
  author = {Moritz, Steffen and Sard{\'a}, Alexis and {Bartz-Beielstein}, Thomas and Zaefferer, Martin and Stork, J{\"o}rg},
  year = {2015},
  month = oct,
  number = {arXiv:1510.03924},
  eprint = {1510.03924},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-09},
  abstract = {Missing values in datasets are a well-known problem and there are quite a lot of R packages offering imputation functions. But while imputation in general is well covered within R, it is hard to find functions for imputation of univariate time series. The problem is, most standard imputation techniques can not be applied directly. Most algorithms rely on inter-attribute correlations, while univariate time series imputation needs to employ time dependencies. This paper provides an overview of univariate time series imputation in general and an in-detail insight into the respective implementations within R packages. Furthermore, we experimentally compare the R functions on different time series using four different ratios of missing data. Our results show that either an interpolation with seasonal kalman filter from the zoo package or a linear interpolation on seasonal loess decomposed data from the forecast package were the most effective methods for dealing with missing data in most of the scenarios assessed in this paper.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/6NXAM6D6/Moritz et al. - 2015 - Comparison of different Methods for Univariate Tim.pdf;/home/stephen/Zotero/storage/IB28KLCU/1510.html},
  keywords = {ComputerScience-OtherComputerScience,Statistics-Applications}
}

@misc{moritzComparisonDifferentMethods2015c,
  title = {Comparison of Different {{Methods}} for {{Univariate Time Series Imputation}} in {{R}}},
  author = {Moritz, Steffen and Sard{\'a}, Alexis and {Bartz-Beielstein}, Thomas and Zaefferer, Martin and Stork, J{\"o}rg},
  year = {2015},
  month = oct,
  number = {arXiv:1510.03924},
  eprint = {1510.03924},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-10-09},
  abstract = {Missing values in datasets are a well-known problem and there are quite a lot of R packages offering imputation functions. But while imputation in general is well covered within R, it is hard to find functions for imputation of univariate time series. The problem is, most standard imputation techniques can not be applied directly. Most algorithms rely on inter-attribute correlations, while univariate time series imputation needs to employ time dependencies. This paper provides an overview of univariate time series imputation in general and an in-detail insight into the respective implementations within R packages. Furthermore, we experimentally compare the R functions on different time series using four different ratios of missing data. Our results show that either an interpolation with seasonal kalman filter from the zoo package or a linear interpolation on seasonal loess decomposed data from the forecast package were the most effective methods for dealing with missing data in most of the scenarios assessed in this paper.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/JG79PJK6/Moritz et al. - 2015 - Comparison of different Methods for Univariate Tim.pdf;/home/stephen/Zotero/storage/ZHWIDWQK/1510.html},
  keywords = {ComputerScience-OtherComputerScience,Statistics-Applications}
}

@book{mullerIntroductionMachineLearning2016,
  title = {Introduction to Machine Learning with {{Python}}: A Guide for Data Scientists},
  shorttitle = {Introduction to Machine Learning with {{Python}}},
  author = {M{\"u}ller, Andreas Christian and Guido, Sarah},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Beijing Boston Farnham Sebastopol Tokyo}},
  isbn = {978-1-4493-6941-5},
  langid = {english},
  file = {/home/stephen/Zotero/storage/98A6U5V3/Andreas C. Müller, Sarah Guido - Introduction to Machine Learning with Python_ A Guide for Data Scientists-O’Reilly Media (2016).pdf;/home/stephen/Zotero/storage/NU8ELT6M/Müller and Guido - 2016 - Introduction to machine learning with Python a gu.pdf}
}

@misc{nicholColoringColorblindness,
  title = {Coloring for {{Colorblindness}}},
  author = {Nichol, David}
}

@article{nicodemoExploratoryDataAnalysis2022,
  title = {Exploratory Data Analysis on Large Data Sets: {{The}} Example of Salary Variation in {{Spanish Social Security Data}}},
  shorttitle = {Exploratory Data Analysis on Large Data Sets},
  author = {Nicodemo, Catia and Satorra, Albert},
  year = {2022},
  month = jul,
  journal = {BRQ Business Research Quarterly},
  volume = {25},
  number = {3},
  pages = {283--294},
  issn = {2340-9444, 2340-9444},
  doi = {10.1177/2340944420957335},
  urldate = {2023-09-25},
  abstract = {New challenges arise in data visualization when the research involves a sizable database. With many data points, classical scatterplots are non-informative due to the cluttering of points. On the contrary, simple plots, such as the boxplot that are of limited use in small samples, offer great potential to facilitate group comparison in the case of an extensive sample. This article presents exploratory data analysis methods useful for inspecting variation across groups in crucial variables and detecting heterogeneity. The exploratory data analysis methods (introduced by Tukey in his seminal book of 1977) encompass a set of statistical tools aimed to extract information from data using simple graphical tools. In this article, some of the exploratory data analysis methods like the boxplot and scatterplot are revisited and enhanced using modern graphical computational devices (as, for example, the heat-map) and their use illustrated with Spanish Social Security data. We explore how earnings vary across several factors like age, gender, type of occupation, and contract, and in particular, the gender gap in salaries is visualized in various dimensions relating to the type of occupation. The exploratory data analysis methods are also applied to assessing and refining competing regressions by plotting residuals-versus-fitted values. The methods discussed should be useful to researchers to assess heterogeneity in data, across-group variation, and classical diagnostic plots of residuals from alternative models fits.             JEL CLASSIFICATION: C55; J01; J08; Y10; C80},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.182Z},
  file = {/home/stephen/Zotero/storage/8RSMDWBE/Nicodemo and Satorra - 2022 - Exploratory data analysis on large data sets The .pdf}
}

@misc{opendataunitDublinbikesAPIHttps2023,
  title = {Dublinbikes {{API}} ({{https://data.gov.ie/dataset/dublinbikes-api)}}},
  author = {{Open Data Unit}},
  year = {2023},
  urldate = {2023-12-30}
}

@incollection{panRepeatedCrossSectionalDesign2019,
  title = {Repeated {{Cross-Sectional Design}}},
  booktitle = {Encyclopedia of {{Gerontology}} and {{Population Aging}}},
  author = {Pan, Xi},
  editor = {Gu, Danan and Dupre, Matthew E.},
  year = {2019},
  pages = {1--5},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-69892-2_578-1},
  urldate = {2023-10-09},
  isbn = {978-3-319-69892-2},
  langid = {english}
}

@inproceedings{pengDataPrepEDATaskCentric2021,
  title = {{{DataPrep}}.{{EDA}}: {{Task-Centric Exploratory Data Analysis}} for {{Statistical Modeling}} in {{Python}}},
  shorttitle = {{{DataPrep}}.{{EDA}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan},
  year = {2021},
  month = jun,
  pages = {2271--2280},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3448016.3457330},
  urldate = {2023-09-20},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.180Z},
  file = {/home/stephen/Zotero/storage/KACA343Q/DataPrepEDA Task Centric Exploratory Data Analysis for Statistical Modeling in Python.pdf},
  keywords = {BigData,DistributedData,EDA,Profiling,Python}
}

@misc{redditRedditAPIHttps2023,
  title = {Reddit {{API}} ({{https://www.reddit.com/dev/api/)}}},
  author = {{Reddit}},
  year = {2023},
  urldate = {2023-12-30}
}

@article{sushkovaStatisticalMethodExploratory2021,
  title = {A {{Statistical Method}} for {{Exploratory Data Analysis Based}} on {{2D}} and {{3D Area}} under {{Curve Diagrams}}: {{Parkinson}}'s {{Disease Investigation}}},
  shorttitle = {A {{Statistical Method}} for {{Exploratory Data Analysis Based}} on {{2D}} and {{3D Area}} under {{Curve Diagrams}}},
  author = {Sushkova, Olga Sergeevna and Morozov, Alexei Alexandrovich and Gabova, Alexandra Vasilievna and Karabanov, Alexei Vyacheslavovich and Illarioshkin, Sergey Nikolaevich},
  year = {2021},
  month = jul,
  journal = {Sensors},
  volume = {21},
  number = {14},
  pages = {4700},
  issn = {1424-8220},
  doi = {10.3390/s21144700},
  urldate = {2023-09-25},
  abstract = {A statistical method for exploratory data analysis based on 2D and 3D area under curve (AUC) diagrams was developed. The method was designed to analyze electroencephalogram (EEG), electromyogram (EMG), and tremorogram data collected from patients with Parkinson's disease. The idea of the method of wave train electrical activity analysis is that we consider the biomedical signal as a combination of the wave trains. The wave train is the increase in the power spectral density of the signal localized in time, frequency, and space. We detect the wave trains as the local maxima in the wavelet spectrograms. We do not consider wave trains as a special kind of signal. The wave train analysis method is different from standard signal analysis methods such as Fourier analysis and wavelet analysis in the following way. Existing methods for analyzing EEG, EMG, and tremor signals, such as wavelet analysis, focus on local time--frequency changes in the signal and therefore do not reveal the generalized properties of the signal. Other methods such as standard Fourier analysis ignore the local time--frequency changes in the characteristics of the signal and, consequently, lose a large amount of information that existed in the signal. The method of wave train electrical activity analysis resolves the contradiction between these two approaches because it addresses the generalized characteristics of the biomedical signal based on local time--frequency changes in the signal. We investigate the following wave train parameters: wave train central frequency, wave train maximal power spectral density, wave train duration in periods, and wave train bandwidth. We have developed special graphical diagrams, named AUC diagrams, to determine what wave trains are characteristic of neurodegenerative diseases. In this paper, we consider the following types of AUC diagrams: 2D and 3D diagrams. The technique of working with AUC diagrams is illustrated by examples of analysis of EMG in patients with Parkinson's disease and healthy volunteers. It is demonstrated that new regularities useful for the high-accuracy diagnosis of Parkinson's disease can be revealed using the method of analyzing the wave train electrical activity and AUC diagrams.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/RPV4BD7N/Sushkova et al. - 2021 - A Statistical Method for Exploratory Data Analysis.pdf}
}

@inproceedings{tangExtractingTopKInsights2017,
  title = {Extracting {{Top-K Insights}} from {{Multi-dimensional Data}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Tang, Bo and Han, Shi and Yiu, Man Lung and Ding, Rui and Zhang, Dongmei},
  year = {2017},
  month = may,
  pages = {1509--1524},
  publisher = {{ACM}},
  address = {{Chicago Illinois USA}},
  doi = {10.1145/3035918.3035922},
  urldate = {2023-09-22},
  isbn = {978-1-4503-4197-4},
  langid = {english},
  file = {/home/stephen/Zotero/storage/4RFQG6T7/Tang et al. - 2017 - Extracting Top-K Insights from Multi-dimensional D.pdf},
  keywords = {AutomatedInsights}
}

@book{thakurApproachingAlmostAny2020,
  title = {Approaching ({{Almost}}) {{Any Machine Learning Problem}}},
  author = {THAKUR, {\relax ABHISHEK}},
  year = {2020},
  publisher = {{ABHISHEK THAKUR}},
  address = {{S.l.}},
  isbn = {978-93-90274-43-7},
  langid = {english},
  annotation = {OCLC: 1368347003},
  file = {/home/stephen/Zotero/storage/A35T5GMT/THAKUR - 2020 - APPROACHING (ALMOST) ANY MACHINE LEARNING PROBLEM.pdf}
}

@inproceedings{tiwariEvolutionIoTData2019,
  title = {Evolution of {{IoT}} \& {{Data Analytics}} Using {{Deep Learning}}},
  booktitle = {2019 {{International Conference}} on {{Computing}}, {{Communication}}, and {{Intelligent Systems}} ({{ICCCIS}})},
  author = {Tiwari, Ratik and Sharma, Nikhil and Kaushik, Ila and Tiwari, Archit and Bhushan, Bharat},
  year = {2019},
  month = oct,
  pages = {418--423},
  publisher = {{IEEE}},
  address = {{Greater Noida, India}},
  doi = {10.1109/ICCCIS48478.2019.8974481},
  urldate = {2024-03-05},
  isbn = {978-1-72814-826-7}
}

@inbook{vohraApacheParquet2016,
  title = {Apache {{Parquet}}},
  booktitle = {Practical {{Hadoop Ecosystem}}},
  author = {Vohra, Deepak},
  year = {2016},
  pages = {325--335},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-2199-0_8},
  urldate = {2024-01-05},
  collaborator = {Vohra, Deepak},
  isbn = {978-1-4842-2198-3 978-1-4842-2199-0},
  langid = {english}
}

@article{wangNewFacialExpression2015,
  title = {New Facial Expression Recognition Based on {{FSVM}} and {{KNN}}},
  author = {Wang, Xiao-Hu and Liu, An and Zhang, Shi-Qing},
  year = {2015},
  month = nov,
  journal = {Optik},
  volume = {126},
  number = {21},
  pages = {3132--3134},
  issn = {00304026},
  doi = {10.1016/j.ijleo.2015.07.073},
  urldate = {2023-10-14},
  langid = {english},
  file = {/home/stephen/Zotero/storage/9GTL32BC/Wang et al. - 2015 - New facial expression recognition based on FSVM an.pdf}
}

@book{weissIntroductoryStatistics2017,
  title = {Introductory Statistics},
  author = {Weiss, Neil A.},
  year = {2017},
  edition = {10th edition, global edition},
  publisher = {{Pearson}},
  address = {{Boston; Columbus; Indianapolis New York}},
  collaborator = {Weiss, Carol A.},
  isbn = {978-1-292-09972-9},
  langid = {english},
  file = {/home/stephen/Zotero/storage/XF5DEB94/Weiss - 2017 - Introductory statistics.pdf}
}

@article{wickhamTidyData2014,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  journal = {Journal of Statistical Software},
  volume = {59},
  number = {10},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  urldate = {2023-10-06},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-06T09:43:38.319Z},
  file = {/home/stephen/Zotero/storage/5Y9L7T2G/Wickham - 2014 - Tidy Data.pdf}
}

@inproceedings{Wirth2000CrispdmTA,
  title = {Crisp-Dm: Towards a Standard Process Modell for Data Mining},
  author = {Wirth, R{\"u}diger and Hipp, Jochen},
  year = {2000}
}

@article{yakoutGuidedDataRepair2011,
  title = {Guided Data Repair},
  author = {Yakout, Mohamed and Elmagarmid, Ahmed K. and Neville, Jennifer and Ouzzani, Mourad and Ilyas, Ihab F.},
  year = {2011},
  month = feb,
  journal = {Proceedings of the VLDB Endowment},
  volume = {4},
  number = {5},
  pages = {279--289},
  issn = {2150-8097},
  doi = {10.14778/1952376.1952378},
  urldate = {2023-09-27},
  abstract = {In this paper we present GDR, a Guided Data Repair framework that incorporates user feedback in the cleaning process to enhance and accelerate existing automatic repair techniques while minimizing user involvement. GDR consults the user on the updates that are most likely to be beneficial in improving data quality. GDR also uses machine learning methods to identify and apply the correct updates directly to the database without the actual involvement of the user on these specific updates. To rank potential updates for consultation by the user, we first group these repairs and quantify the utility of each group using the decision-theory concept of value of information (VOI). We then apply active learning to order updates within a group based on their ability to improve the learned model. User feedback is used to repair the database and to adaptively refine the training set for the model. We empirically evaluate GDR on a real-world dataset and show significant improvement in data quality using our user guided repairing process. We also, assess the trade-off between the user efforts and the resulting data quality.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/RUH7C79W/Yakout et al. - 2011 - Guided data repair.pdf}
}

@inproceedings{yangUseCasePerformance2020,
  title = {Use {{Case}} and {{Performance Analyses}} for {{Missing Data Imputation Methods}} in {{Big Data Analytics}}},
  booktitle = {Proceedings of 2020 6th {{International Conference}} on {{Computing}} and {{Data Engineering}}},
  author = {Yang, Lan and Chiang, Jason Amaro},
  year = {2020},
  month = jan,
  pages = {107--111},
  publisher = {{ACM}},
  address = {{Sanya China}},
  doi = {10.1145/3379247.3379270},
  urldate = {2023-09-20},
  isbn = {978-1-4503-7673-0},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.178Z},
  file = {/home/stephen/Zotero/storage/57VBYQYU/Yang and Chiang - 2020 - Use Case and Performance Analyses for Missing Data.pdf},
  keywords = {Imputation,MissingData}
}

@inproceedings{yanTesseraDiscretizingData2021,
  title = {Tessera: {{Discretizing Data Analysis Workflows}} on a {{Task Level}}},
  shorttitle = {Tessera},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yan, Jing Nathan and Gu, Ziwei and Rzeszotarski, Jeffrey M},
  year = {2021},
  month = may,
  pages = {1--15},
  publisher = {{ACM}},
  address = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445728},
  urldate = {2023-09-22},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {/home/stephen/Zotero/storage/GLSVDPPB/Yan et al. - 2021 - Tessera Discretizing Data Analysis Workflows on a.pdf}
}

@inproceedings{Yee1996AdvantagesAD,
  title = {Advantages and Disadvantages : Longitudinal vs. Repeated Cross-Section Surveys},
  author = {Yee, Julie L. and Niemeier, Debbie A.},
  year = {1996}
}

@book{zakiDataMiningMachine2020,
  title = {Data {{Mining}} and {{Machine Learning}}: {{Fundamental Concepts}} and {{Algorithms}}},
  shorttitle = {Data {{Mining}} and {{Machine Learning}}},
  author = {Zaki, Mohammed J. and Meira, Jr, Wagner},
  year = {2020},
  month = jan,
  edition = {2},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108564175},
  urldate = {2023-09-20},
  isbn = {978-1-108-56417-5 978-1-108-47398-9},
  file = {/home/stephen/Zotero/storage/4Z673CIN/Zaki and Meira, Jr - 2020 - Data Mining and Machine Learning Fundamental Conc.pdf}
}

@article{zhaoReviewRecentAdvances2021,
  title = {A {{Review}} of {{Recent Advances}} of {{Binary Neural Networks}} for {{Edge Computing}}},
  author = {Zhao, Wenyu and Ma, Teli and Gong, Xuan and Zhang, Baochang and Doermann, David},
  year = {2021},
  month = mar,
  journal = {IEEE Journal on Miniaturization for Air and Space Systems},
  volume = {2},
  number = {1},
  eprint = {2011.14824},
  primaryclass = {cs},
  pages = {25--35},
  issn = {2576-3164},
  doi = {10.1109/JMASS.2020.3034205},
  urldate = {2024-02-27},
  abstract = {Edge computing is promising to become one of the next hottest topics in artificial intelligence because it benefits various evolving domains such as real-time unmanned aerial systems, industrial applications, and the demand for privacy protection. This paper reviews recent advances on binary neural network (BNN) and 1-bit CNN technologies that are well suitable for front-end, edge-based computing. We introduce and summarize existing work and classify them based on gradient approximation, quantization, architecture, loss functions, optimization method, and binary neural architecture search. We also introduce applications in the areas of computer vision and speech recognition and discuss future applications for edge computing.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/K22XIFCL/Zhao et al. - 2021 - A Review of Recent Advances of Binary Neural Netwo.pdf;/home/stephen/Zotero/storage/KTVKGMFA/2011.html},
  keywords = {ComputerScience-ArtificialIntelligence,ComputerScience-MachineLearning}
}

@article{zharmagambetovExperimentalComparisonOld2019,
  title = {An {{Experimental Comparison}} of {{Old}} and {{New Decision Tree Algorithms}}},
  author = {Zharmagambetov, Arman and Hada, Suryabhan Singh and {Carreira-Perpi{\~n}{\'a}n}, Miguel {\'A}. and Gabidolla, Magzhan},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1911.03054},
  urldate = {2023-09-28},
  abstract = {This paper presents a detailed comparison of a recently proposed algorithm for optimizing decision trees, tree alternating optimization (TAO), with other popular, established algorithms. We compare their performance on a number of classification and regression datasets of various complexity, different size and dimensionality, across different performance factors: accuracy and tree size (in terms of the number of leaves or the depth of the tree). We find that TAO achieves higher accuracy in nearly all datasets, often by a large margin.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/home/stephen/Zotero/storage/8XXAJLK3/Zharmagambetov et al. - 2019 - An Experimental Comparison of Old and New Decision.pdf},
  keywords = {FOS:ComputerAndInformationSciences,MachineLearning(Cs.LG),MachineLearning(Stat.ML)}
}
