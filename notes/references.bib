@article{abukmeilSurveyUnsupervisedGenerative2022,
  title = {A {{Survey}} of {{Unsupervised Generative Models}} for {{Exploratory Data Analysis}} and {{Representation Learning}}},
  author = {Abukmeil, Mohanad and Ferrari, Stefano and Genovese, Angelo and Piuri, Vincenzo and Scotti, Fabio},
  year = {2022},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {5},
  pages = {1--40},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3450963},
  urldate = {2023-09-26},
  abstract = {For more than a century, the methods for data representation and the exploration of the intrinsic structures of data have developed remarkably and consist of supervised and unsupervised methods. However, recent years have witnessed the flourishing of big data, where typical dataset dimensions are high and the data can come in messy, incomplete, unlabeled, or corrupted forms. Consequently, discovering the hidden structure buried inside such data becomes highly challenging. From this perspective, exploratory data analysis plays a substantial role in learning the hidden structures that encompass the significant features of the data in an ordered manner by extracting patterns and testing hypotheses to identify anomalies. Unsupervised generative learning models are a class of machine learning models characterized by their potential to reduce the dimensionality, discover the exploratory factors, and learn representations without any predefined labels; moreover, such models can generate the data from the reduced factors' domain. The beginner researchers can find in this survey the recent unsupervised generative learning models for the purpose of data exploration and learning representations; specifically, this article covers three families of methods based on their usage in the era of big data: blind source separation, manifold learning, and neural networks, from shallow to deep architectures.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.889Z},
  file = {/home/stephen/Zotero/storage/NUCDKM3Z/Abukmeil et al. - 2022 - A Survey of Unsupervised Generative Models for Exp.pdf}
}

@article{aliHeartDiseasePrediction2021,
  title = {Heart Disease Prediction Using Supervised Machine Learning Algorithms: {{Performance}} Analysis and Comparison},
  shorttitle = {Heart Disease Prediction Using Supervised Machine Learning Algorithms},
  author = {Ali, Md Mamun and Paul, Bikash Kumar and Ahmed, Kawsar and Bui, Francis M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
  year = {2021},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {136},
  pages = {104672},
  issn = {00104825},
  doi = {10.1016/j.compbiomed.2021.104672},
  urldate = {2023-09-20},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.179Z},
  file = {/home/stephen/Zotero/storage/CSG4QWE8/Ali et al. - 2021 - Heart disease prediction using supervised machine .pdf},
  keywords = {MachineLearning,SupervisedLearning}
}

@article{Bertsimas2017FromPM,
  title = {From Predictive Methods to Missing Data Imputation: {{An}} Optimization Approach},
  author = {Bertsimas, Dimitris and Pawlowski, Colin and Zhuo, Ying Daisy},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {196:1-196:39},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.893Z},
  file = {/home/stephen/Zotero/storage/X8HSFH2E/Bertsimas et al. - 2017 - From predictive methods to missing data imputation.pdf}
}

@misc{buitinckAPIDesignMachine2013,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  shorttitle = {{{API}} Design for Machine Learning Software},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  year = {2013},
  month = sep,
  number = {arXiv:1309.0238},
  eprint = {1309.0238},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/6H4539S5/Buitinck et al. - 2013 - API design for machine learning software experien.pdf;/home/stephen/Zotero/storage/QCNF79QH/API design for machine learning software.pdf;/home/stephen/Zotero/storage/GJDK7Q48/1309.html},
  keywords = {APIDesign,MachineLearning}
}

@book{burgerIntroductionMachineLearning2018,
  title = {Introduction to Machine Learning with {{R}}: Rigorous Mathematical Analysis},
  shorttitle = {Introduction to Machine Learning with {{R}}},
  author = {Burger, Scott V.},
  year = {2018},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Beijing Boston Farnham Sebastopol Tokyo}},
  abstract = {Machine learning can be a difficult subject if you're not familiar with the basics. With this book, you'll get a solid foundation of introductory principles used in machine learning with the statistical programming language R. You'll start with the basics like regression, then move into more advanced topics like neural networks, and finally delve into the frontier of machine learning in the R world with packages like Caret. By developing a familiarity with topics like understanding the difference between regression and classification models, you'll be able to solve an array of machine learning problems. Knowing when to use a specific model or not can mean the difference between a highly accurate model and a completely useless one. This book provides copious examples to build a working knowledge of machine learning. Understand the major parts of machine learning algorithms Recognize how machine learning can be used to solve a problem in a simple manner Figure out when to use certain machine learning algorithms versus others Learn how to operationalize algorithms with cutting edge packages},
  isbn = {978-1-4919-7644-9},
  langid = {english},
  file = {/home/stephen/Zotero/storage/9B2CTQJF/Scott V. Burger - Introduction to Machine Learning with R. Rigorous Mathematical Analysis-Oâ€™Reilly (2018).pdf;/home/stephen/Zotero/storage/X2QPID99/Burger - 2018 - Introduction to machine learning with R rigorous .pdf}
}

@article{chiangInfoCleanProtectingSensitive2017,
  title = {{{InfoClean}}: {{Protecting Sensitive Information}} in {{Data Cleaning}}},
  shorttitle = {{{InfoClean}}},
  author = {Chiang, Fei and Gairola, Dhruv},
  year = {2017},
  month = dec,
  journal = {Journal of Data and Information Quality},
  volume = {9},
  number = {4},
  pages = {1--26},
  issn = {1936-1955, 1936-1963},
  doi = {10.1145/3190577},
  urldate = {2023-09-26},
  abstract = {Data quality has become a pervasive challenge for organizations as they wrangle with large, heterogeneous datasets to extract value. Given the proliferation of sensitive and confidential information, it is crucial to consider data privacy concerns during the data cleaning process. For example, in medical database applications, varying levels of privacy are enforced across the attribute values. Attributes such as a patient's country or city of residence may be less sensitive than the patient's prescribed medication. Traditional data cleaning techniques assume the data is openly accessible, without considering the differing levels of information sensitivity. In this work, we take the first steps toward a data cleaning model that integrates privacy as part of the data cleaning process. We present a privacy-aware data cleaning framework that differentiates the information content among the attribute values during the data cleaning process to resolve data inconsistencies while minimizing the amount of information disclosed. Our data repair algorithm includes a set of data disclosure operations that considers the information content of the underlying attribute values, while maximizing data utility. Our evaluation using real datasets shows that our algorithm scales well, and achieves improved performance and comparable repair accuracy against existing data cleaning solutions.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.890Z},
  file = {/home/stephen/Zotero/storage/H3I4K7ZC/Chiang and Gairola - 2017 - InfoClean Protecting Sensitive Information in Dat.pdf}
}

@article{chuDistributedDataDeduplication2016,
  title = {Distributed Data Deduplication},
  author = {Chu, Xu and Ilyas, Ihab F. and Koutris, Paraschos},
  year = {2016},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {11},
  pages = {864--875},
  issn = {2150-8097},
  doi = {10.14778/2983200.2983203},
  urldate = {2023-09-27},
  abstract = {Data deduplication refers to the process of identifying tuples in a relation that refer to the same real world entity. The complexity of the problem is inherently quadratic with respect to the number of tuples, since a similarity value must be computed for every pair of tuples. To avoid comparing tuple pairs that are obviously non-duplicates, blocking techniques are used to divide the tuples into blocks and only tuples within the same block are compared. However, even with the use of blocking, data deduplication remains a costly problem for large datasets. In this paper, we show how to further speed up data deduplication by leveraging parallelism in a shared-nothing computing environment. Our main contribution is a distribution strategy, called Dis-Dedup, that minimizes the maximum workload across all worker nodes and provides strong theoretical guarantees. We demonstrate the effectiveness of our proposed strategy by performing extensive experiments on both synthetic datasets with varying block size distributions, as well as real world datasets.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/22QX8R32/Chu et al. - 2016 - Distributed data deduplication.pdf}
}

@article{chuQualitativeDataCleaning2016,
  title = {Qualitative Data Cleaning},
  author = {Chu, Xu and Ilyas, Ihab F.},
  year = {2016},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {9},
  number = {13},
  pages = {1605--1608},
  issn = {2150-8097},
  doi = {10.14778/3007263.3007320},
  urldate = {2023-09-26},
  abstract = {Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.             Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.             In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning "big data" in terms of scale and distribution.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.891Z},
  file = {/home/stephen/Zotero/storage/XP984QYI/Chu and Ilyas - 2016 - Qualitative data cleaning.pdf}
}

@misc{cuiDataSiteProactiveVisual2018,
  title = {{{DataSite}}: {{Proactive Visual Data Exploration}} with {{Computation}} of {{Insight-based Recommendations}}},
  shorttitle = {{{DataSite}}},
  author = {Cui, Zhe and Badam, Sriram Karthik and Yal{\c c}in, Adil and Elmqvist, Niklas},
  year = {2018},
  month = sep,
  number = {arXiv:1802.08621},
  eprint = {1802.08621},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-22},
  abstract = {Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.},
  archiveprefix = {arxiv},
  file = {/home/stephen/Zotero/storage/IP6X7PCH/DataSite  Proactive Visual Data.pdf;/home/stephen/Zotero/storage/L4SIDND9/Cui et al. - 2018 - DataSite Proactive Visual Data Exploration with C.pdf;/home/stephen/Zotero/storage/GFY3TLZD/1802.html},
  keywords = {AutomatedInsights,DataExploration,Recommendation}
}

@book{dangetiStatisticsMachineLearning2017,
  title = {Statistics for Machine Learning: Techniques for Exploring Supervised, Unsupervised, and Reinforcement Learning Models with {{Python}} and {{R}}},
  shorttitle = {Statistics for Machine Learning},
  author = {Dangeti, Pratap},
  year = {2017},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, UK}},
  isbn = {978-1-78829-575-8},
  langid = {english},
  file = {/home/stephen/Zotero/storage/ABA8TNW4/Dangeti - 2017 - Statistics for machine learning techniques for ex.pdf}
}

@article{dengEfficientKNNClassification2016,
  title = {Efficient {{kNN}} Classification Algorithm for Big Data},
  author = {Deng, Zhenyun and Zhu, Xiaoshu and Cheng, Debo and Zong, Ming and Zhang, Shichao},
  year = {2016},
  month = jun,
  journal = {Neurocomputing},
  volume = {195},
  pages = {143--148},
  issn = {09252312},
  doi = {10.1016/j.neucom.2015.08.112},
  urldate = {2023-09-26},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-10-03T14:41:05.887Z},
  file = {/home/stephen/Documents/college/research/papers/Efficient kNN classification algorithm for big data.pdf}
}

@article{deutchExplainEDExplanationsEDA2020,
  title = {{{ExplainED}}: Explanations for {{EDA}} Notebooks},
  shorttitle = {{{ExplainED}}},
  author = {Deutch, Daniel and Gilad, Amir and Milo, Tova and Somech, Amit},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {2917--2920},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415508},
  urldate = {2023-09-22},
  abstract = {Exploratory Data Analysis (EDA) is an essential yet highly demanding task. To get a head start before exploring a new dataset, data scientists often prefer to view existing               EDA notebooks               - illustrative exploratory sessions that were created by fellow data scientists who examined the same dataset and shared their notebooks via online platforms. Unfortunately, creating an illustrative, well-documented notebook is cumbersome and time-consuming, therefore users sometimes share their notebook without explaining their exploratory steps and their results. Such notebooks are difficult to follow and to understand.                          To address this, we present ExplainED, a system that automatically attaches explanations to views in EDA notebooks. ExplainED analyzes each view in order to detect what elements thereof are particularly interesting, and produces a corresponding textual explanation. The explanations are generated by first evaluating the interestingness of the given view using several measures capturing different interestingness facets, then computing the Shapely values of the elements in the view, w.r.t. the interestingness measure yielding the highest score. These Shapely values are then used to guide the generation of the textual explanation.             We demonstrate the usefulness of the explanations generated by ExplainED on real-life, undocumented EDA notebooks.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/T4K936ED/Deutch et al. - 2020 - ExplainED explanations for EDA notebooks.pdf},
  keywords = {EDA}
}

@inproceedings{dingQuickInsightsQuickAutomatic2019,
  title = {{{QuickInsights}}: {{Quick}} and {{Automatic Discovery}} of {{Insights}} from {{Multi-Dimensional Data}}},
  shorttitle = {{{QuickInsights}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Ding, Rui and Han, Shi and Xu, Yong and Zhang, Haidong and Zhang, Dongmei},
  year = {2019},
  month = jun,
  pages = {317--332},
  publisher = {{ACM}},
  address = {{Amsterdam Netherlands}},
  doi = {10.1145/3299869.3314037},
  urldate = {2023-09-22},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {/home/stephen/Zotero/storage/XKUH44Z2/Ding et al. - 2019 - QuickInsights Quick and Automatic Discovery of In.pdf},
  keywords = {AutomatedInsights}
}

@book{downeyThinkStats2012,
  title = {Think Stats},
  author = {Downey, Allen B.},
  year = {2012},
  series = {Probability and Statistics for Programmers},
  edition = {1. ed., 4. release},
  publisher = {{O'Reilly}},
  address = {{Beijing}},
  isbn = {978-1-4493-0711-0},
  langid = {english},
  file = {/home/stephen/Zotero/storage/J8WTXV5R/Downey - 2012 - Think stats.pdf}
}

@book{geronHandsonMachineLearning2019,
  title = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}: Concepts, Tools, and Techniques to Build Intelligent Systems},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2019},
  edition = {Second edition},
  publisher = {{O'Reilly Media, Inc}},
  address = {{Beijing [China] ; Sebastopol, CA}},
  isbn = {978-1-4920-3264-9},
  lccn = {QA76.73.P98 G45 2019},
  file = {/home/stephen/Zotero/storage/E8USP4M9/GÃ©ron - 2019 - Hands-on machine learning with Scikit-Learn, Keras.pdf},
  keywords = {ArtificialIntelligence,MachineLearning,Python(ComputerProgramLanguage),TensorFlow}
}

@article{hassanatSolvingProblemParameter2014,
  title = {Solving the {{Problem}} of the {{K Parameter}} in the {{KNN Classifier Using}} an {{Ensemble Learning Approach}}},
  author = {Hassanat, Ahmad Basheer and Abbadi, Mohammad Ali and Altarawneh, Ghada Awad and Alhasanat, Ahmad Ali},
  year = {2014},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1409.0919},
  urldate = {2023-09-28},
  abstract = {This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/home/stephen/Zotero/storage/XEDXSKBP/Hassanat et al. - 2014 - Solving the Problem of the K Parameter in the KNN .pdf},
  keywords = {FOS:ComputerAndInformationSciences,MachineLearning(Cs.LG)}
}

@book{jamesIntroductionStatisticalLearning2013,
  title = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  series = {Springer {{Texts}} in {{Statistics}}},
  volume = {103},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-7138-7},
  urldate = {2023-09-20},
  isbn = {978-1-4614-7137-0 978-1-4614-7138-7},
  file = {/home/stephen/Zotero/storage/GZYJ4IKC/(Springer Texts in Statistics) Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani - An Introduction to Statistical Learning with Applications in R-Springer (2014).pdf;/home/stephen/Zotero/storage/IBSYJFME/James et al. - 2013 - An Introduction to Statistical Learning.pdf}
}

@article{koumarelasDataPreparationDuplicate2020,
  title = {Data {{Preparation}} for {{Duplicate Detection}}},
  author = {Koumarelas, Ioannis and Jiang, Lan and Naumann, Felix},
  year = {2020},
  month = sep,
  journal = {Journal of Data and Information Quality},
  volume = {12},
  number = {3},
  pages = {1--24},
  issn = {1936-1955, 1936-1963},
  doi = {10.1145/3377878},
  urldate = {2023-09-20},
  abstract = {Data errors represent a major issue in most application workflows. Before any important task can take place, a certain data quality has to be guaranteed by eliminating a number of different errors that may appear in data. Typically, most of these errors are fixed with data preparation methods, such as whitespace removal. However, the particular error of duplicate records, where multiple records refer to the same entity, is usually eliminated independently with specialized techniques. Our work is the first to bring these two areas together by applying data preparation operations under a systematic approach prior to performing duplicate detection.             Our process workflow can be summarized as follows: It begins with the user providing as input a sample of the gold standard, the actual dataset, and optionally some constraints to domain-specific data preparations, such as address normalization. The preparation selection operates in two consecutive phases. First, to vastly reduce the search space of ineffective data preparations, decisions are made based on the improvement or worsening of pair similarities. Second, using the remaining data preparations an iterative leave-one-out classification process removes preparations one by one and determines the redundant preparations based on the achieved area under the precision-recall curve (AUC-PR). Using this workflow, we manage to improve the results of duplicate detection up to 19\% in AUC-PR.},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.176Z},
  file = {/home/stephen/Zotero/storage/2Q24ST9L/Koumarelas et al. - 2020 - Data Preparation for Duplicate Detection.pdf},
  keywords = {DataCleaning,EDA}
}

@book{laroseDiscoveringKnowledgeData2005,
  title = {Discovering Knowledge in Data: An Introduction to Data Mining},
  shorttitle = {Discovering Knowledge in Data},
  author = {Larose, Daniel T.},
  year = {2005},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, NJ}},
  isbn = {978-0-471-66657-8},
  langid = {english},
  file = {/home/stephen/Zotero/storage/699LC4WH/(Wiley Series on Methods and Applications in Data Mining) Daniel T. Larose, Chantel D. Larose - Discovering Knowledge in data An Introduction to Data Mining-Wiley ( 2014).pdf;/home/stephen/Zotero/storage/6MN277H8/Larose - 2005 - Discovering knowledge in data an introduction to .pdf}
}

@article{liBayesianApproachEstimating2009,
  title = {A {{Bayesian Approach}} for {{Estimating}} and {{Replacing Missing Categorical Data}}},
  author = {Li, Xiaobai},
  year = {2009},
  journal = {ACM J. Data Inf. Qual.},
  volume = {1},
  pages = {3:1-3:11},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.175Z},
  file = {/home/stephen/Zotero/storage/ZAHZDSMS/Li - 2009 - A Bayesian Approach for Estimating and Replacing M.pdf},
  keywords = {Bayesian,CategoricalData,DataCleaning,Imputation,MissingData}
}

@inproceedings{linBigIN4InstantInteractive2018,
  title = {{{BigIN4}}: {{Instant}}, {{Interactive Insight Identification}} for {{Multi-Dimensional Big Data}}},
  shorttitle = {{{BigIN4}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Lin, Qingwei and Ke, Weichen and Lou, Jian-Guang and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Zhou, Ziyi and Qiao, Bo and Zhang, Dongmei},
  year = {2018},
  month = jul,
  pages = {547--555},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3219867},
  urldate = {2023-09-22},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  file = {/home/stephen/Zotero/storage/VLQLTFXM/Lin et al. - 2018 - BigIN4 Instant, Interactive Insight Identificatio.pdf},
  keywords = {AutomatedInsights}
}

@book{mullerIntroductionMachineLearning2016,
  title = {Introduction to Machine Learning with {{Python}}: A Guide for Data Scientists},
  shorttitle = {Introduction to Machine Learning with {{Python}}},
  author = {M{\"u}ller, Andreas Christian and Guido, Sarah},
  year = {2016},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Beijing Boston Farnham Sebastopol Tokyo}},
  isbn = {978-1-4493-6941-5},
  langid = {english},
  file = {/home/stephen/Zotero/storage/98A6U5V3/Andreas C. MÃ¼ller, Sarah Guido - Introduction to Machine Learning with Python_ A Guide for Data Scientists-Oâ€™Reilly Media (2016).pdf;/home/stephen/Zotero/storage/NU8ELT6M/MÃ¼ller and Guido - 2016 - Introduction to machine learning with Python a gu.pdf}
}

@article{nicodemoExploratoryDataAnalysis2022,
  title = {Exploratory Data Analysis on Large Data Sets: {{The}} Example of Salary Variation in {{Spanish Social Security Data}}},
  shorttitle = {Exploratory Data Analysis on Large Data Sets},
  author = {Nicodemo, Catia and Satorra, Albert},
  year = {2022},
  month = jul,
  journal = {BRQ Business Research Quarterly},
  volume = {25},
  number = {3},
  pages = {283--294},
  issn = {2340-9444, 2340-9444},
  doi = {10.1177/2340944420957335},
  urldate = {2023-09-25},
  abstract = {New challenges arise in data visualization when the research involves a sizable database. With many data points, classical scatterplots are non-informative due to the cluttering of points. On the contrary, simple plots, such as the boxplot that are of limited use in small samples, offer great potential to facilitate group comparison in the case of an extensive sample. This article presents exploratory data analysis methods useful for inspecting variation across groups in crucial variables and detecting heterogeneity. The exploratory data analysis methods (introduced by Tukey in his seminal book of 1977) encompass a set of statistical tools aimed to extract information from data using simple graphical tools. In this article, some of the exploratory data analysis methods like the boxplot and scatterplot are revisited and enhanced using modern graphical computational devices (as, for example, the heat-map) and their use illustrated with Spanish Social Security data. We explore how earnings vary across several factors like age, gender, type of occupation, and contract, and in particular, the gender gap in salaries is visualized in various dimensions relating to the type of occupation. The exploratory data analysis methods are also applied to assessing and refining competing regressions by plotting residuals-versus-fitted values. The methods discussed should be useful to researchers to assess heterogeneity in data, across-group variation, and classical diagnostic plots of residuals from alternative models fits.             JEL CLASSIFICATION: C55; J01; J08; Y10; C80},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.182Z},
  file = {/home/stephen/Zotero/storage/8RSMDWBE/Nicodemo and Satorra - 2022 - Exploratory data analysis on large data sets The .pdf}
}

@inproceedings{pengDataPrepEDATaskCentric2021,
  title = {{{DataPrep}}.{{EDA}}: {{Task-Centric Exploratory Data Analysis}} for {{Statistical Modeling}} in {{Python}}},
  shorttitle = {{{DataPrep}}.{{EDA}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan},
  year = {2021},
  month = jun,
  pages = {2271--2280},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3448016.3457330},
  urldate = {2023-09-20},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.180Z},
  file = {/home/stephen/Zotero/storage/KACA343Q/DataPrepEDA Task Centric Exploratory Data Analysis for Statistical Modeling in Python.pdf},
  keywords = {BigData,DistributedData,EDA,Profiling,Python}
}

@article{sushkovaStatisticalMethodExploratory2021,
  title = {A {{Statistical Method}} for {{Exploratory Data Analysis Based}} on {{2D}} and {{3D Area}} under {{Curve Diagrams}}: {{Parkinson}}'s {{Disease Investigation}}},
  shorttitle = {A {{Statistical Method}} for {{Exploratory Data Analysis Based}} on {{2D}} and {{3D Area}} under {{Curve Diagrams}}},
  author = {Sushkova, Olga Sergeevna and Morozov, Alexei Alexandrovich and Gabova, Alexandra Vasilievna and Karabanov, Alexei Vyacheslavovich and Illarioshkin, Sergey Nikolaevich},
  year = {2021},
  month = jul,
  journal = {Sensors},
  volume = {21},
  number = {14},
  pages = {4700},
  issn = {1424-8220},
  doi = {10.3390/s21144700},
  urldate = {2023-09-25},
  abstract = {A statistical method for exploratory data analysis based on 2D and 3D area under curve (AUC) diagrams was developed. The method was designed to analyze electroencephalogram (EEG), electromyogram (EMG), and tremorogram data collected from patients with Parkinson's disease. The idea of the method of wave train electrical activity analysis is that we consider the biomedical signal as a combination of the wave trains. The wave train is the increase in the power spectral density of the signal localized in time, frequency, and space. We detect the wave trains as the local maxima in the wavelet spectrograms. We do not consider wave trains as a special kind of signal. The wave train analysis method is different from standard signal analysis methods such as Fourier analysis and wavelet analysis in the following way. Existing methods for analyzing EEG, EMG, and tremor signals, such as wavelet analysis, focus on local time\textendash frequency changes in the signal and therefore do not reveal the generalized properties of the signal. Other methods such as standard Fourier analysis ignore the local time\textendash frequency changes in the characteristics of the signal and, consequently, lose a large amount of information that existed in the signal. The method of wave train electrical activity analysis resolves the contradiction between these two approaches because it addresses the generalized characteristics of the biomedical signal based on local time\textendash frequency changes in the signal. We investigate the following wave train parameters: wave train central frequency, wave train maximal power spectral density, wave train duration in periods, and wave train bandwidth. We have developed special graphical diagrams, named AUC diagrams, to determine what wave trains are characteristic of neurodegenerative diseases. In this paper, we consider the following types of AUC diagrams: 2D and 3D diagrams. The technique of working with AUC diagrams is illustrated by examples of analysis of EMG in patients with Parkinson's disease and healthy volunteers. It is demonstrated that new regularities useful for the high-accuracy diagnosis of Parkinson's disease can be revealed using the method of analyzing the wave train electrical activity and AUC diagrams.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/RPV4BD7N/Sushkova et al. - 2021 - A Statistical Method for Exploratory Data Analysis.pdf}
}

@inproceedings{tangExtractingTopKInsights2017,
  title = {Extracting {{Top-K Insights}} from {{Multi-dimensional Data}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Tang, Bo and Han, Shi and Yiu, Man Lung and Ding, Rui and Zhang, Dongmei},
  year = {2017},
  month = may,
  pages = {1509--1524},
  publisher = {{ACM}},
  address = {{Chicago Illinois USA}},
  doi = {10.1145/3035918.3035922},
  urldate = {2023-09-22},
  isbn = {978-1-4503-4197-4},
  langid = {english},
  file = {/home/stephen/Zotero/storage/4RFQG6T7/Tang et al. - 2017 - Extracting Top-K Insights from Multi-dimensional D.pdf},
  keywords = {AutomatedInsights}
}

@book{thakurApproachingAlmostAny2020,
  title = {Approaching ({{Almost}}) {{Any Machine Learning Problem}}},
  author = {THAKUR, {\relax ABHISHEK}},
  year = {2020},
  publisher = {{ABHISHEK THAKUR}},
  address = {{S.l.}},
  isbn = {978-93-90274-43-7},
  langid = {english},
  annotation = {OCLC: 1368347003},
  file = {/home/stephen/Zotero/storage/A35T5GMT/THAKUR - 2020 - APPROACHING (ALMOST) ANY MACHINE LEARNING PROBLEM.pdf}
}

@book{weissIntroductoryStatistics2017,
  title = {Introductory Statistics},
  author = {Weiss, Neil A.},
  year = {2017},
  edition = {10th edition, global edition},
  publisher = {{Pearson}},
  address = {{Boston; Columbus; Indianapolis New York}},
  collaborator = {Weiss, Carol A.},
  isbn = {978-1-292-09972-9},
  langid = {english},
  file = {/home/stephen/Zotero/storage/XF5DEB94/Weiss - 2017 - Introductory statistics.pdf}
}

@article{yakoutGuidedDataRepair2011,
  title = {Guided Data Repair},
  author = {Yakout, Mohamed and Elmagarmid, Ahmed K. and Neville, Jennifer and Ouzzani, Mourad and Ilyas, Ihab F.},
  year = {2011},
  month = feb,
  journal = {Proceedings of the VLDB Endowment},
  volume = {4},
  number = {5},
  pages = {279--289},
  issn = {2150-8097},
  doi = {10.14778/1952376.1952378},
  urldate = {2023-09-27},
  abstract = {In this paper we present GDR, a Guided Data Repair framework that incorporates user feedback in the cleaning process to enhance and accelerate existing automatic repair techniques while minimizing user involvement. GDR consults the user on the updates that are most likely to be beneficial in improving data quality. GDR also uses machine learning methods to identify and apply the correct updates directly to the database without the actual involvement of the user on these specific updates. To rank potential updates for consultation by the user, we first group these repairs and quantify the utility of each group using the decision-theory concept of value of information (VOI). We then apply active learning to order updates within a group based on their ability to improve the learned model. User feedback is used to repair the database and to adaptively refine the training set for the model. We empirically evaluate GDR on a real-world dataset and show significant improvement in data quality using our user guided repairing process. We also, assess the trade-off between the user efforts and the resulting data quality.},
  langid = {english},
  file = {/home/stephen/Zotero/storage/RUH7C79W/Yakout et al. - 2011 - Guided data repair.pdf}
}

@inproceedings{yangUseCasePerformance2020,
  title = {Use {{Case}} and {{Performance Analyses}} for {{Missing Data Imputation Methods}} in {{Big Data Analytics}}},
  booktitle = {Proceedings of 2020 6th {{International Conference}} on {{Computing}} and {{Data Engineering}}},
  author = {Yang, Lan and Chiang, Jason Amaro},
  year = {2020},
  month = jan,
  pages = {107--111},
  publisher = {{ACM}},
  address = {{Sanya China}},
  doi = {10.1145/3379247.3379270},
  urldate = {2023-09-20},
  isbn = {978-1-4503-7673-0},
  langid = {english},
  annotation = {Read\_Status: Read Read\_Status\_Date: 2023-09-25T08:43:07.178Z},
  file = {/home/stephen/Zotero/storage/57VBYQYU/Yang and Chiang - 2020 - Use Case and Performance Analyses for Missing Data.pdf},
  keywords = {Imputation,MissingData}
}

@inproceedings{yanTesseraDiscretizingData2021,
  title = {Tessera: {{Discretizing Data Analysis Workflows}} on a {{Task Level}}},
  shorttitle = {Tessera},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yan, Jing Nathan and Gu, Ziwei and Rzeszotarski, Jeffrey M},
  year = {2021},
  month = may,
  pages = {1--15},
  publisher = {{ACM}},
  address = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445728},
  urldate = {2023-09-22},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  file = {/home/stephen/Zotero/storage/GLSVDPPB/Yan et al. - 2021 - Tessera Discretizing Data Analysis Workflows on a.pdf}
}

@book{zakiDataMiningMachine2020,
  title = {Data {{Mining}} and {{Machine Learning}}: {{Fundamental Concepts}} and {{Algorithms}}},
  shorttitle = {Data {{Mining}} and {{Machine Learning}}},
  author = {Zaki, Mohammed J. and Meira, Jr, Wagner},
  year = {2020},
  month = jan,
  edition = {2},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781108564175},
  urldate = {2023-09-20},
  isbn = {978-1-108-56417-5 978-1-108-47398-9},
  file = {/home/stephen/Zotero/storage/4Z673CIN/Zaki and Meira, Jr - 2020 - Data Mining and Machine Learning Fundamental Conc.pdf}
}

@article{zharmagambetovExperimentalComparisonOld2019,
  title = {An {{Experimental Comparison}} of {{Old}} and {{New Decision Tree Algorithms}}},
  author = {Zharmagambetov, Arman and Hada, Suryabhan Singh and {Carreira-Perpi{\~n}{\'a}n}, Miguel {\'A}. and Gabidolla, Magzhan},
  year = {2019},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1911.03054},
  urldate = {2023-09-28},
  abstract = {This paper presents a detailed comparison of a recently proposed algorithm for optimizing decision trees, tree alternating optimization (TAO), with other popular, established algorithms. We compare their performance on a number of classification and regression datasets of various complexity, different size and dimensionality, across different performance factors: accuracy and tree size (in terms of the number of leaves or the depth of the tree). We find that TAO achieves higher accuracy in nearly all datasets, often by a large margin.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/home/stephen/Zotero/storage/8XXAJLK3/Zharmagambetov et al. - 2019 - An Experimental Comparison of Old and New Decision.pdf},
  keywords = {FOS:ComputerAndInformationSciences,MachineLearning(Cs.LG),MachineLearning(Stat.ML)}
}
