# Machine learning:

Keywords: #MachineLearning #SupervisedLearning #DecisionTrees #GINI #DataNormalization #sklearn #KNN
## Finishing off Tutorial 1

How to choose the optimal value of KNN - is there a formal method? Something like AIC that penalises higher values of K but also takes train & test accuracy into account?
- [ ] Find a paper on numerical strategies for choosing optimal values of N in KNN

sklearn.classification_report  definitions - f score, weighted avg, macro avg etc.
- [ ] Read documentation on sklearn.classification_report

Why normalize / standardize data? Why isn't it done with iris?
- [ ] Read up on reasons why normalizing data is performed

Why was petal width removed in Tutorial 1? Apparently due to analysis of the heatmap but I'm unsure why exactly

## RF & Decision Trees

Review Decision Tree growth algorithm differences - Hunt's, CART, etc.
- [ ] Find a paper comparing difference Decision Tree algorithms

Categorical splitting justification - gini impurity, entropy etc.
- GINI index is an impurity calculation - for each node in a split you get a measure based on the probability of seeing each class in the split nodes. If the weighted gini between child nodes is greater than the parent node's gini, you have positive gain, so a split is justified.

- Entropy is an alternative split criterion 
- [ ]  What are the differences between GINI and Entropy?

- Error is not typically used because of linear behaviour - sharp inflection / gradual slope to the maximum gives small differences where the model should be unsure?
- [ ] Why is GINI / Entropy preferred to Error?

_fit_transform() vs transform()_ - fit_transform fits its parameters to the sklearn object, then transforms the input. transform uses an already fitted transformer, so test data should only be transformed by transform, not fit_transform - https://stackoverflow.com/questions/59101623/how-to-use-fit-and-transform-for-training-and-testing-data-with-standardscaler

